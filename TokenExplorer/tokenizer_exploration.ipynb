{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# set warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# import modules and classes\n",
    "from TokenExplorer.commons.utils.downloads import DownloadManager\n",
    "from TokenExplorer.commons.utils.plotter import DataPlotter\n",
    "from TokenExplorer.commons.utils.analyzer.explorer import ExploreTokenizers\n",
    "from TokenExplorer.commons.constants import BENCHMARK_FIGURES_PATH \n",
    "from TokenExplorer.commons.logger import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load tokenizers and text dataset\n",
    "\n",
    "Download a series of tokenizers from Hugging Face and save them in /tokenizers. Then, download text corpora for tokenizer benchmarking and save them in /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = DownloadManager()\n",
    "tokenizers = manager.tokenizer_download()\n",
    "datasets = manager.dataset_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizers vocabulary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check length of tokenizers vocabulary, by using two methods: 1) extraction of the embedded vocabulary and 2) decoding by using the indexes of the embedded vocabulary. The idea is to compare the obtained sets of words and spot any possible discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer = ExploreTokenizers(tokenizers)\n",
    "explorer.vocabulary_report()\n",
    "explorer.plot_vocabulary_size(BENCHMARK_FIGURES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze distribution of token by characters length using histograms and boxplots, comparing both the distribution of word lengths from the vocabulary or obtained through decoding of indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer.histogram_tokens_length(BENCHMARK_FIGURES_PATH)\n",
    "explorer.boxplot_tokens_length(BENCHMARK_FIGURES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare number of subwords versus normal words, comparing both the distribution of word lengths from the vocabulary or obtained through decoding of indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer.subwords_vs_words(BENCHMARK_FIGURES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizers benchmark on text datasets\n",
    "\n",
    "Tokenizers are benchmarked on the wikitext-103-v1 dataset. The benchmark consists in comparing text pre- and post-tokenization and calculate words and tokens count, average length and by-item lengths, and ratio between tokens and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = DataPlotter()\n",
    "df_benchmarks = plotter.benchmark_data\n",
    "df_NSL = plotter.NSL_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a series of metrics to evaluate the performance of the tokenizers on the given text. \n",
    "\n",
    "1) Token to words ratio is shown to evaluate number of generate tokens versus number of words in text (by document)\n",
    "2) Average character length of tokens versus average length of words (average by document)\n",
    "3) Bytes per Token, and is calculated by dividing the number of UTF-8 bytes by the number of tokens produced by the tokenizer on a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplots of token to word ratio by document for each tokenizer\n",
    "plotter.benchmarks_boxplot(df_benchmarks, BENCHMARK_FIGURES_PATH, x_vals='Tokenizer', \n",
    "                           y_vals='Tokens/words ratio', y_label='Token to word ratio (by document)',\n",
    "                           hue=None, title='Tokens to words ratio by tokenizer')\n",
    "\n",
    "# create a df with melted AVG values columns to plot them using seaborn\n",
    "# specify tokenizer name as hue parameter\n",
    "df_melt = pd.melt(df_benchmarks, id_vars='Tokenizer', value_vars=['AVG words length', 'AVG tokens length'],\n",
    "                  var_name='Item type', value_name='AVG length')\n",
    "plotter.benchmarks_boxplot(df_benchmarks, BENCHMARK_FIGURES_PATH, x_vals='Tokenizer', \n",
    "                           y_vals='AVG tokens length', y_label='Token to word ratio (by document)',\n",
    "                           hue=None, title='Average token vs word length by tokenizer')\n",
    "\n",
    "# create a df with melted AVG values columns to plot them using seaborn\n",
    "# specify tokenizer name as hue parameter\n",
    "plotter.benchmarks_boxplot(df_benchmarks, BENCHMARK_FIGURES_PATH, x_vals='Tokenizer', \n",
    "                           y_vals='Bytes per token', y_label='',\n",
    "                           hue=None, title='Bytes (utf-8) per token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Normalized Sequence Length (NSL), comparing the compression of our custom tokenizer with respect to each of the hugging face tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_NSL is not None:\n",
    "    df_NSL = df_NSL[df_NSL['Tokenizer'] != 'custom tokenizer']\n",
    "plotter.benchmarks_boxplot(df_NSL, BENCHMARK_FIGURES_PATH, x_vals='Tokenizer', \n",
    "                           y_vals='NSL', y_label='', hue=None, title='Normalized Sequence Length (NSL)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aquarius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
